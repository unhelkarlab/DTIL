# @package _global_

alg_name: mahil
stream_training: True
n_update_rounds: 256 # only when stream_training=False
demo_latent_infer_interval: 1000 # only when stream_training=True
# update strategy
mahil_update_strategy: 1 # 1: always update both / 2: update in order / 3: update alternatively
mahil_tx_after_pi: True
mahil_alter_update_n_pi_tx: [10, 5]
mahil_order_update_pi_ratio: 0.5
# tx
mahil_tx_activation: relu
mahil_tx_hidden_critic: [128, 128]
mahil_tx_optimizer_lr_critic: 3.e-4
mahil_tx_method_loss: value
mahil_tx_method_regularize: False
mahil_tx_method_div: chi
mahil_tx_init_temp: 1e-4
mahil_tx_clip_grad_val: null
mahil_tx_num_actor_update: 1
mahil_tx_num_critic_update: 1
# pi
mahil_pi_activation: relu
# pi - Q-network
mahil_pi_hidden_critic: [128, 128]
mahil_pi_optimizer_lr_critic: 3.e-4
mahil_pi_method_loss: value
mahil_pi_method_regularize: True
mahil_pi_method_div: ""
mahil_pi_num_critic_update: 1
mahil_pi_single_critic: True
# pi - actor
mahil_pi_hidden_policy: [128, 128]
mahil_pi_optimizer_lr_policy: 3.e-4
mahil_pi_log_std_bounds: [-5., 2.]
mahil_pi_num_actor_update: 1
mahil_pi_clip_grad_val: null
mahil_pi_bounded_actor: True
mahil_pi_use_nn_logstd: True
mahil_pi_clamp_action_logstd: False # True: use clamp() / False: use tanh
# pi - alpha
mahil_pi_optimizer_lr_alpha: 3.e-4
mahil_pi_init_temp: 1e-2
mahil_pi_learn_temp: False
